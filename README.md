# DataLinq.NET

> **We make Data fit for C#.**

From local files to cloud scale ‚Äî **LINQ all the way down**.  
Let IntelliSense and the compiler do the work.

```diff
- df.filter(pl.col("ammount") > 1000)   # Typo? Runtime error.
+ .Where(o => o.Amount > 1000)          // Typo? Won't compile. ‚úì
```

[![License](https://img.shields.io/badge/License-Apache%202.0-green)](https://github.com/improveTheWorld/DataLinq.NET/blob/main/LICENSE)
[![Tests](https://img.shields.io/badge/Tests-924%20passing-brightgreen)](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/COVERAGE.md)
[![Coverage](https://img.shields.io/badge/Code%20Coverage-60%25-yellowgreen)](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/COVERAGE.md)
[![NuGet](https://img.shields.io/nuget/v/DataLinq.NET.svg?label=DataLinq.NET)](https://www.nuget.org/packages/DataLinq.NET/)
[![NuGet Downloads](https://img.shields.io/nuget/dt/DataLinq.NET.svg)](https://www.nuget.org/packages/DataLinq.NET/)

```bash
# Install via NuGet
dotnet add package DataLinq.NET --version 1.0.0
```

---

## Table of Contents

1. [Sound Familiar?](#1-sound-familiar)
2. [Three Simple Rules](#2-three-simple-rules)
3. [Everything is a Stream](#3-everything-is-a-stream)
4. [Quick Start](#4-quick-start)
5. [Documentation](#5-documentation)
6. [Community & Support](#6-community--support)

---

## 1. Sound Familiar?

.NET developers know the story ‚Äî You write a clean, type-safe data processor in C# ‚Äî It works perfectly on your dev machine ‚Äî Then reality hits:

1.  **The Data Grows**: 
    - **10 MB**: `List<T>` works fine.
    - **10 GB**: `OutOfMemoryException`. You rewrite using `StreamReader`.
    - **10 TB**: You abandon C# for Spark/SQL. You lose type safety and duplicate logic.

2.  **The Logic Tangles**:
    - New requirements mean new `if/else` branches.
    - You loop over the same data 5 times to handle 5 different cases.
    - The code becomes spaghetti, and the data lifecycle becomes a black box.

3.  **The Source Fragments**:
    - Today it's a CSV file. Tomorrow it's a REST API. Next week it's a Kafka Stream.
    - For each source, you write different adapter code.
    - You end up with a **"Code Salad"**: mixed abstractions, different error handling, and no reuse.

**DataLinq.NET was built to stop this cycle:**

- ‚úÖ **Unified API** ‚Äî Same code for CSV, JSON, Kafka, Spark
- ‚úÖ **Constant memory** ‚Äî Stream billions of rows without `OutOfMemoryException` ([see benchmarks](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/Benchmarks.md))
- ‚úÖ **No spaghetti** ‚Äî Declarative `Cases` pattern replaces nested `if/else`
- ‚úÖ **Pure C#** ‚Äî LINQ all the way down

> [!TIP]
> **Define the *what*. DataLinq.NET handles the *how*.**

--- 

## 2. Three Simple Rules

DataLinq.NET is more than a framework ‚Äî it defines a pattern to process data.

```mermaid
graph LR
    S[**S**ink] --> U[**U**nify]
    U --> P[**P**rocess]
    P --> R[**R**oute]
    R --> A[**A**pply]
    
    style S fill:#f9f,stroke:#333,stroke-width:2px
    style A fill:#bbf,stroke:#333,stroke-width:2px
```

We call this the **SUPRA** pattern ‚Äî the name comes from gathering the first letter of each stage: **S**ink, **U**nify, **P**rocess, **R**oute, **A**pply.

> [!NOTE]
> The SUPRA pattern ensures memory stays constant and items flow one at a time. [Read the SUPRA-Pattern Guide ‚Üí](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/DataLinq-SUPRA-Pattern.md)

To achieve the SUPRA pattern, you'll have to follow these rules:

1. **Sink First** ‚Äî Buffer and normalize at the edge, never in the middle.
2. **Flow Lazy** ‚Äî Items stream one by one. Constant memory.
3. **Route Declaratively** ‚Äî No more `if/else` spaghetti.

DataLinq.NET provides all the ready-to-use blocks to natively apply these rules.

---
## 3. Everything is a Stream

DataLinq.NET provides tools to abstract the *source* of data from the *processing*. Use these to make every data source an `IAsyncEnumerable<T>` stream ‚Äî the essence of the "Unified API" ‚Äî same LINQ operators, same processing logic, regardless of origin.

[See Integration Patterns Guide ‚Üí](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/Integration-Patterns-Guide.md)

| Source Type | Pattern | Output |
|-------------|---------|--------|
| **EF Core (SQL Server, PostgreSQL, etc.)** | `.AsAsyncEnumerable()` | `IAsyncEnumerable<T>` |
| **JSON/CSV/YAML Files** | `Read.Json<T>()` / `Read.Csv<T>()` | `IAsyncEnumerable<T>` |
| **REST APIs** | `.Poll()` + `.SelectMany()` | `IAsyncEnumerable<T>` |
| **Kafka / RabbitMQ / WebSocket** | Wrap + `.WithBoundedBuffer()` | `IAsyncEnumerable<T>` |
| **Snowflake** *(Premium)* | `Snowflake.Connect().Read.Table<T>()` | `SnowflakeQuery<T>` |
| **Apache Spark** *(Premium)* | `Spark.Connect().Read.Table<T>()` | `SparkQuery<T>` |


> [!IMPORTANT]
> Any `IAsyncEnumerable<T>` source integrates natively.

### Examples

Already using Entity Framework Core? DataLinq.NET plugs right in:

```csharp
// EF Core ‚Äî Native support
await dbContext.Orders.AsAsyncEnumerable()
    .Where(o => o.Amount > 100)
    .WriteCsv("orders.csv");
```
*   ‚úÖ EF Core handles database access
*   ‚úÖ DataLinq.NET handles processing logic
*   ‚úÖ Works with SQL Server, PostgreSQL, MySQL, SQLite

Need to integrate REST APIs or message queues? Use polling and buffering:

```csharp
// REST API ‚Äî Poll and flatten
var orders = (() => httpClient.GetFromJsonAsync<Order[]>("/api/orders"))
    .Poll(TimeSpan.FromSeconds(5), token)
    .SelectMany(batch => batch.ToAsyncEnumerable());

// Kafka/WebSocket ‚Äî Wrap in async iterator + buffer
var kafkaStream = ConsumeKafka(token).WithBoundedBuffer(1024);
```



### High-Performance Streaming File Readers

DataLinq.NET provides high-performance file readers: no Reflection on the hot path; expression trees are compiled once and cached.

*   **4x faster** than standard reflection-based creation ([benchmark results ‚Üí](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/Benchmarks.md))
*   **Zero allocation overhead** ‚Äî same 48 bytes as native `new()` instantiation
*   Handles CSV, JSON, and YAML files generically.

We carefully crafted an intuitive, fully-featured readers API with advanced error handling ‚Äî all while streaming row-by-row.

> [!TIP]
> The streaming row-by-row approach ‚Äî absent in most other frameworks ‚Äî is the cornerstone of DataLinq.NET's constant memory usage.

[Materialization Quick Reference ‚Üí](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/Materialization-Quick-Reference.md) | [Data Reading Guide ‚Üí](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/DataLinq-Data-Reading-Infrastructure.md)


### LINQ Extensions

DataLinq.NET implements additional LINQ extensions to make every data loop composable‚Äîeven side-effect loops.

- **Independent implementation** ‚Äî Re-implemented `IAsyncEnumerable` methods without depending on `System.Linq.Async`
- **Clear terminal vs non-terminal separation** ‚Äî Terminal methods (`Do()`, `Display()`) force execution; non-terminal methods (`ForEach()`, `Select()`, `Where()`) stay lazy

[See Extension Methods API Reference ‚Üí](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/Extension-Methods-API-Reference.md)

### Cases/SelectCase/ForEachCase

We've extended standard LINQ with custom operators for declarative branching. Using `Cases`, `SelectCase`, and `ForEachCase`, you can replace complex nested `if/else` blocks with an optimized, single-pass dispatch tree ‚Äî while remaining fully composable.

[See Cases Pattern Guide ‚Üí](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/Cases-Pattern.md)

### Multi-Source Stream Merging
This is the "U" (Unify) step of the SUPRA pattern ‚Äî "absorb many sources into one stream."

```csharp
var unifiedStream = new UnifiedStream<Log>()
    .Unify(fileLogs, "archive")
    .Unify(apiLogs, "live")
    .Unify(dbLogs, "backup");
// Result: A single IAsyncEnumerable<Log> you can query
```

[See Stream Merging Guide ‚Üí](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/Stream-Merging.md)

### Debug with Spy()

Insert observation points anywhere in your pipeline without changing data flow. Because `Spy()` is fully composable, you can add or remove traces by simply commenting a line ‚Äî no code rewriting required.

```csharp
await data
    .Where(...)
    .Spy("After filtering")       // üëà See items flow through
    .Select(...)
    .Spy("After transformation")
    .ForEach(...)                 // üëà Side-effect iteration, still composable
    .Do();                        // üëà Force execution (no output needed)
```

> ‚ö†Ô∏è **Note:** Due to lazy execution, output from multiple `Spy()` calls appears interleaved 
> (item-by-item), not grouped by stage. This preserves the streaming nature of the pipeline.

### Go Parallel When You Need To

Need to parallelize CPU-intensive or I/O-bound work? DataLinq.NET provides parallel counterparts that work just like their sequential equivalents ‚Äî still lazy, still composable:

```csharp
// Parallel sync processing
await data.AsParallel()
    .Select(item => ExpensiveCompute(item))
    .ForEach(item => WriteToDb(item))
    .Do();

// Parallel async processing
await asyncStream.AsParallel()
    .WithMaxConcurrency(8)
    .Select(async item => await FetchAsync(item))
    .Do();
```

[See ParallelAsyncQuery API Reference ‚Üí](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/ParallelAsyncQuery-API-Reference.md) | [Parallel Processing Guide ‚Üí](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/Parallel-Processing.md) | [Extension Methods ‚Üí](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/Extension-Methods-API-Reference.md)

### Scale to the cloud *(Premium)*
If you hit the limit of local computing power, DataLinq.NET lets you **seamlessly** scale to the cloud with **LINQ-to-Spark & Snowflake**.
Your C# lambda expressions are decompiled at runtime and translated into **native Spark/SQL execution plans**.
*   ‚úÖ No data transfer to client
*   ‚úÖ Execution happens on the cluster
*   ‚úÖ Full type safety

[LINQ-to-Spark Guide ‚Üí](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/LINQ-to-Spark.md) | [LINQ-to-Snowflake Guide ‚Üí](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/LINQ-to-Snowflake.md)


---

## 4. Quick Start

### Prerequisites
- [.NET 8.0 SDK](https://dotnet.microsoft.com/download/dotnet/8.0) or later

### Installation

**Via NuGet (Recommended):**
```bash
dotnet add package DataLinq.NET --version 1.0.0
```

**Or clone the repository:**
```bash
git clone https://github.com/improveTheWorld/DataLinq.NET
cd DataLinq.NET
```

### Run the Usage Examples
```bash
dotnet run --project tests/IntegrationTests/DataLinq.UsageExamples/DataLinq.App.UsageExamples.csproj
```

Or open the full solution in Visual Studio 2022:
```
DataLinq.Net.sln
```

### Your First Pipeline
```csharp
using DataLinq;

// A complete, memory-efficient pipeline in 10 lines
await Read.Csv<Order>("orders.csv")
    .Cases(
        o => o.Amount > 1000, 
        o => o.CustomerType == "VIP"
    )
    .SelectCase(
        highValue => ProcessHighValue(highValue),
        vip => ProcessVip(vip)
    )
    .AllCases()
    .WriteJson("output.json");
```

### Advanced: One Logic, Multiple Targets

Your business rule is: *"Flag high-value transactions from international customers."*

```csharp
// 1. DEVELOPMENT: Read from a local CSV file
await Read.Csv<Order>("orders.csv")
    .Cases(o => o.Amount > 10000, o => o.IsInternational) // üëà Your Logic
    .SelectCase(...) 
    .AllCases()
    .WriteCsv("output.csv");

// 2. PRODUCTION: Merge multiple async streams
await new UnifiedStream<Order>()
    .Unify(ordersApi, "api")
    .Unify(ordersDb, "db")
    .Cases(o => o.Amount > 10000, o => o.IsInternational) // üëà SAME Logic
    .SelectCase(...)
    .AllCases()
    .WriteJson("output.json");

// 3. CLOUD: Query Snowflake Data Warehouse
// Filters and aggregations execute on the server
using var sfContext = Snowflake.Connect(account, user, password, database, warehouse);
await sfContext.Read.Table<Order>("orders")
    .Where(o => o.Year == 2024)
    .Cases(o => o.Amount > 10000, o => o.IsInternational) // üëà SAME Logic
    .SelectCase(...)
    .ToList();

// 4. SCALE: Run on Apache Spark (Petabyte Scale)
// Translates your C# Expression Tree to native Spark orchestration
using var sparkContext = Spark.Connect("spark://master:7077", "MyApp");
sparkContext.Read.Table<Order>("sales.orders")
    .Where(o => o.Amount > 10000)
    .Cases(o => o.Amount > 50000, o => o.IsInternational) // üëà SAME Logic
    .SelectCase(...)
    .AllCases()
    .WriteParquet("s3://data/output");
```

---

## 5. Documentation

| Topic | Description |
|-------|-------------|
| üè∞ **[SUPRA Pattern](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/DataLinq-SUPRA-Pattern.md)** | The SUPRA Pattern deep dive |
| üîÄ **[Cases Pattern](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/Cases-Pattern.md)** | The Cases/SelectCase/ForEachCase Engine |
| üìñ **[Data Reading](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/DataLinq-Data-Reading-Infrastructure.md)** | Reading CSV, JSON, YAML, Text |
| üéØ **[Materialization Guide](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/Materialization-Quick-Reference.md)** | Design classes for CSV, JSON, YAML, Snowflake, Spark |
| ‚úçÔ∏è **[Data Writing](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/DataLinq-Data-Writing-Infrastructure.md)** | Writing CSV, JSON, YAML, Text |
| üåä **[Stream Merging](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/Stream-Merging.md)** | UnifiedStream & Multi-Source Streams |
| üîÑ **[Polling & Buffering](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/Polling-Buffering.md)** | Data acquisition patterns |
| üî• **[Big Data](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/LINQ-to-Spark.md)** | Running C# on Apache Spark |
| ‚ùÑÔ∏è **[Snowflake](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/LINQ-to-Snowflake.md)** | LINQ-to-Snowflake Provider |
| üöÄ **[Performance](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/ObjectMaterializer.md)** | The Zero-Allocation Engine |
| üìã **[API Reference](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/API-Reference.md)** | Complete API Documentation |
| üß© **[Extension Methods](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/Extension-Methods-API-Reference.md)** | IEnumerable/IAsyncEnumerable/Parallel API Matrix |
| üîå **[Integration Patterns](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/Integration-Patterns-Guide.md)** | HTTP, Kafka, EF Core, WebSocket examples |
| ‚ö° **[Parallel Processing](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/Parallel-Processing.md)** | ParallelQuery & ParallelAsyncQuery |
| ‚ö° **[ParallelAsyncQuery](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/ParallelAsyncQuery-API-Reference.md)** | Parallel async processing API |
| üß™ **[Test Coverage](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/COVERAGE.md)** | Coverage Reports (60% Weighted) |
| üó∫Ô∏è **[Roadmap](https://github.com/improveTheWorld/DataLinq.NET/blob/main/docs/Roadmap.md)** | Future Enterprise Connectors |

---

## 6. Community & Support

*   **Issues**: [GitHub Issues](https://github.com/improveTheWorld/DataLinq.NET/issues)

*   **Email**: [support@get-datalinq.net](mailto:support@get-datalinq.net)

**DataLinq.NET** ‚Äî *Sink the chaos. Let the rest flow pure.* üöÄ
